{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we experiment with analyzing all the github data for 2015 using [dask](http://dask.pydata.org/en/latest/) ([github](https://github.com/continuumIO/dask)) for analysis.\n",
    "\n",
    "\n",
    "**Dask** is a tool for out-of-core, parallel data analysis. We will use [dask.bag](http://dask.pydata.org/en/latest/bag.html), which provides an api for operations on unordered lists (like sets but with duplicates). It is useful for semi-structured data like JSON blobs or log files. More blogposts about dask can be found [here](http://www.continuum.io/blog/tags/dask) or [here](http://matthewrocklin.com/blog/tags.html#dask-ref)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github Archive Data on Google Cloud Storage\n",
    "\n",
    "We took data from [githubarchive.com](githubarchive.com), from January 2015 and put it in Google Cloud Storage so we can get free transfers between there, and google compute, which runs binder.\n",
    "\n",
    "Lets inspect the data first so we can find something to analyze and learn the data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm github-data.tar\n",
    "!wget  https://storage.googleapis.com/blaze-data/github-data/github-data.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tar xf github-data.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!du -h data/  # 4.6GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Data with `dask.bag`\n",
    "\n",
    "We have approximately 4.6 GB of data. One file per hour, averaging around 7.8 MB each (compressed). So we make a dask bag with the data and inspect it to figure out the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "import dask.bag as db\n",
    "import ujson as json\n",
    "\n",
    "# take one file from the bucket load it as a json object, not gz decompression\n",
    "# happens automatically at compute time.\n",
    "b = db.from_filenames('data/2015-*.json.gz').map(json.loads)\n",
    "b.npartitions  # number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first = b.take(1)[0]  # take the first json object from the file\n",
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first.keys()  # top level keys in this json object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type looks interesting. What are possible types and how often does each occur? We can inspect this with `dask.bag.frequencies`. But this takes longer because it requires a read of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    res = b.pluck('type').frequencies().compute()\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Committers\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most events are pushes, that is not surprising. Lets ask \"Who pushes the most?\".\n",
    "\n",
    "We do this by filtering out `PushEvent`s. Then we count the frequencies of usernames for the pushes. Then take the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pushes = b.filter(lambda x: x['type'] == 'PushEvent')  # filter out the push events\n",
    "names = pushes.pluck('actor').pluck('login') # get the login names\n",
    "top_5 = names.frequencies().topk(5, key=lambda (name, count): count)  # List top 5 pushers\n",
    "with ProgressBar():\n",
    "    res = top_5.compute()  # run the above computations\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These users *pushed* the most, but push can have multiple commits. So we can ask \"who pushed the most *commits*?\".\n",
    "\n",
    "We can figure this out by grouping by username, then summing the number of commits from every push, for each user. More technically speaking, we want to `GroupBy` on usernames, so for each username we get a list their of PushEvents. Then reduce each `PushEvent` by taking a `count` of their commits. Then reducing these `count`s by `sum`ing them for each user. So we are grouping then reducing.\n",
    "\n",
    "However there are algorithms for grouping and reducing simultaneously which avoid expensive shuffle operations and are much faster. In dask bag we have `foldby`. Analogous methods: [`toolz.reduceby`]( http://toolz.readthedocs.org/en/latest/api.html#toolz.itertoolz.reduceby), and in pyspark [`RDD.combineByKey`](https://spark.apache.org/docs/latest/api/python/pyspark.html?#pyspark.RDD.combineByKey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_logins(x):\n",
    "    \"\"\"The key for foldby, like a groupby key. Get the username from a PushEvent\"\"\"\n",
    "    return x['actor']['login']\n",
    "\n",
    "def binop(total, x):\n",
    "    \"\"\"Count the number of commits in a PushEvent\"\"\"\n",
    "    return total + len(x['payload']['commits'])\n",
    "\n",
    "def combine(total1, total2):\n",
    "    \"\"\"This combines commit counts from PushEvents\"\"\"\n",
    "    return total1 + total2\n",
    "\n",
    "commits = pushes.foldby(get_logins, binop, initial=0, combine=combine)\n",
    "top_commits = commits.topk(5, key=lambda (name, count): count)\n",
    "with ProgressBar():\n",
    "    res = top_commits.compute()\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
